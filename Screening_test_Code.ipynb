{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "nF_wyQmfliI-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YZkBJnKyloxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('target1.csv')"
      ],
      "metadata": {
        "id": "KarZh1Yjk7IB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "1dMim3HXlr1o",
        "outputId": "b24d6cc4-56da-4c84-9f03-5f5292c554eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width      species\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-af04af44-9c7b-41ca-aa9d-b71cc1df6b22\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-af04af44-9c7b-41ca-aa9d-b71cc1df6b22')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-af04af44-9c7b-41ca-aa9d-b71cc1df6b22 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-af04af44-9c7b-41ca-aa9d-b71cc1df6b22');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4e3286f1-b283-4b07-9ded-a0bbb25fad8a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e3286f1-b283-4b07-9ded-a0bbb25fad8a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4e3286f1-b283-4b07-9ded-a0bbb25fad8a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 150,\n  \"fields\": [\n    {\n      \"column\": \"sepal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8280661279778629,\n        \"min\": 4.3,\n        \"max\": 7.9,\n        \"num_unique_values\": 35,\n        \"samples\": [\n          6.2,\n          4.5,\n          5.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4335943113621737,\n        \"min\": 2.0,\n        \"max\": 4.4,\n        \"num_unique_values\": 23,\n        \"samples\": [\n          2.3,\n          4.0,\n          3.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_length\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7644204199522617,\n        \"min\": 1.0,\n        \"max\": 6.9,\n        \"num_unique_values\": 43,\n        \"samples\": [\n          6.7,\n          3.8,\n          3.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal_width\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7631607417008414,\n        \"min\": 0.1,\n        \"max\": 2.5,\n        \"num_unique_values\": 22,\n        \"samples\": [\n          0.2,\n          1.2,\n          1.3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Iris-setosa\",\n          \"Iris-versicolor\",\n          \"Iris-virginica\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Read the features (which are column names in the csv) and figure out what missing imputation needs to be applied and apply that to the columns loaded in a dataframe**"
      ],
      "metadata": {
        "id": "_tZ_S65GsJuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['sepal_length'].fillna(df['sepal_length'].mean(), inplace=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykyxj4r_mr0e",
        "outputId": "04ee5b11-2af1-41b0-9c5d-76c7946bd892"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-f863bf8827e3>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df['sepal_length'].fillna(df['sepal_length'].mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Compute feature reduction based on input. See the screenshot below where there can be No Reduction, Corr with Target, Tree-based, PCA. Please make sure you write code so that all options can work. If we rerun your code with a different Json it should work if we switch No Reduction to say PCA.**"
      ],
      "metadata": {
        "id": "q5XsN65FsWOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
        "\n",
        "def apply_feature_reduction(X, y, config):\n",
        "    \"\"\"\n",
        "    Apply feature reduction based on the provided configuration.\n",
        "\n",
        "    Parameters:\n",
        "    X (pd.DataFrame): Input features\n",
        "    y (pd.Series): Target variable\n",
        "    config (dict): Feature reduction configuration\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Reduced feature set\n",
        "    dict: Information about the feature reduction process\n",
        "    \"\"\"\n",
        "    method = config[\"feature_reduction_method\"]\n",
        "    method_config = config[method]\n",
        "    info = {\"method\": method}\n",
        "\n",
        "    if not method_config[\"is_selected\"]:\n",
        "        return X, {\"method\": \"None\", \"reason\": \"Selected method is not enabled in config\"}\n",
        "\n",
        "    if method == \"No Reduction\":\n",
        "        # Simply select top N features (no actual reduction, just filtering)\n",
        "        num_features = min(method_config[\"num_of_features_to_keep\"], X.shape[1])\n",
        "        selected_features = X.columns[:num_features]\n",
        "        reduced_X = X[selected_features]\n",
        "        info[\"num_features_kept\"] = num_features\n",
        "        info[\"selected_features\"] = list(selected_features)\n",
        "\n",
        "    elif method == \"Correlation with target\":\n",
        "        # Select features based on correlation with target\n",
        "        num_features = method_config[\"num_of_features_to_keep\"]\n",
        "        if num_features <= 0:\n",
        "            return X, {\"method\": method, \"reason\": \"num_of_features_to_keep must be positive\"}\n",
        "\n",
        "        # Determine if regression or classification problem\n",
        "        problem_type = 'regression' if pd.api.types.is_numeric_dtype(y) else 'classification'\n",
        "        score_func = f_regression if problem_type == 'regression' else f_classif\n",
        "\n",
        "        selector = SelectKBest(score_func=score_func, k=num_features)\n",
        "        selector.fit(X, y)\n",
        "\n",
        "        selected_features = X.columns[selector.get_support()]\n",
        "        reduced_X = X[selected_features]\n",
        "        info[\"num_features_kept\"] = num_features\n",
        "        info[\"selected_features\"] = list(selected_features)\n",
        "        info[\"scores\"] = selector.scores_.tolist()\n",
        "        info[\"pvalues\"] = selector.pvalues_.tolist()\n",
        "\n",
        "    elif method == \"Tree-based\":\n",
        "        # Select features using tree-based importance\n",
        "        num_features = method_config[\"num_of_features_to_keep\"]\n",
        "        depth = method_config[\"depth_of_trees\"]\n",
        "        num_trees = method_config[\"num_of_trees\"]\n",
        "\n",
        "        if num_features <= 0:\n",
        "            return X, {\"method\": method, \"reason\": \"num_of_features_to_keep must be positive\"}\n",
        "\n",
        "        # Determine if regression or classification problem\n",
        "        problem_type = 'regression' if pd.api.types.is_numeric_dtype(y) else 'classification'\n",
        "\n",
        "        if problem_type == 'regression':\n",
        "            model = RandomForestRegressor(\n",
        "                n_estimators=num_trees,\n",
        "                max_depth=depth,\n",
        "                random_state=42\n",
        "            )\n",
        "        else:\n",
        "            model = RandomForestClassifier(\n",
        "                n_estimators=num_trees,\n",
        "                max_depth=depth,\n",
        "                random_state=42\n",
        "            )\n",
        "\n",
        "        model.fit(X, y)\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[-num_features:]\n",
        "\n",
        "        selected_features = X.columns[indices]\n",
        "        reduced_X = X[selected_features]\n",
        "        info[\"num_features_kept\"] = num_features\n",
        "        info[\"selected_features\"] = list(selected_features)\n",
        "        info[\"feature_importances\"] = importances.tolist()\n",
        "        info[\"model_params\"] = model.get_params()\n",
        "\n",
        "    elif method == \"Principal Component Analysis\":\n",
        "        # Apply PCA\n",
        "        num_features = method_config[\"num_of_features_to_keep\"]\n",
        "\n",
        "        if num_features <= 0:\n",
        "            return X, {\"method\": method, \"reason\": \"num_of_features_to_keep must be positive\"}\n",
        "\n",
        "        pca = PCA(n_components=num_features)\n",
        "        reduced_X = pd.DataFrame(\n",
        "            pca.fit_transform(X),\n",
        "            columns=[f\"PC_{i+1}\" for i in range(num_features)],\n",
        "            index=X.index\n",
        "        )\n",
        "        info[\"num_features_kept\"] = num_features\n",
        "        info[\"explained_variance_ratio\"] = pca.explained_variance_ratio_.tolist()\n",
        "        info[\"components\"] = pca.components_.tolist()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown feature reduction method: {method}\")\n",
        "\n",
        "    return reduced_X, info\n",
        "\n",
        "# Example usage:\n",
        "config = {\n",
        "    \"feature_reduction_method\": \"No Reduction\",\n",
        "    \"No Reduction\": {\n",
        "        \"is_selected\": True,\n",
        "        \"num_of_features_to_keep\": 5\n",
        "    },\n",
        "    \"Correlation with target\": {\n",
        "        \"is_selected\": False,\n",
        "        \"num_of_features_to_keep\": 0\n",
        "    },\n",
        "    \"Tree-based\": {\n",
        "        \"is_selected\": False,\n",
        "        \"num_of_features_to_keep\": 0,\n",
        "        \"depth_of_trees\": 0,\n",
        "        \"num_of_trees\": 0\n",
        "    },\n",
        "    \"Principal Component Analysis\": {\n",
        "        \"is_selected\": False,\n",
        "        \"num_of_features_to_keep\": 0\n",
        "    }\n",
        "}\n",
        "\n",
        "# Assuming X is your feature DataFrame and y is your target Series\n",
        "# reduced_X, reduction_info = apply_feature_reduction(X, y, config)\n"
      ],
      "metadata": {
        "id": "Q6YY5t5EreIt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4) Parse the Json and make the model objects (using sklean) that can handle what is required in the “prediction_type” specified in the JSON (See #1 where “prediction_type” is specified). Keep in mind not to pick models that don’t apply for the prediction_type.**"
      ],
      "metadata": {
        "id": "lw5-Cr4ytxrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "def create_logistic_regression_model(config, prediction_type):\n",
        "    \"\"\"\n",
        "    Create a LogisticRegression model based on the provided configuration and prediction type.\n",
        "\n",
        "    Parameters:\n",
        "    config (dict): Configuration dictionary for Logistic Regression\n",
        "    prediction_type (str): Either \"classification\" or \"regression\"\n",
        "\n",
        "    Returns:\n",
        "    tuple: (model object, message) - message indicates any warnings or adjustments made\n",
        "    \"\"\"\n",
        "    if not config[\"is_selected\"]:\n",
        "        return None, \"Model is not selected in configuration\"\n",
        "\n",
        "    if prediction_type != \"classification\":\n",
        "        return None, f\"LogisticRegression is not suitable for {prediction_type} (only for classification)\"\n",
        "\n",
        "    # Extract parameters from config with defaults\n",
        "    params = {\n",
        "        \"n_jobs\": config.get(\"parallelism\", 1),\n",
        "        \"max_iter\": np.random.randint(config.get(\"min_iter\", 30),\n",
        "                                     config.get(\"max_iter\", 50)),\n",
        "        \"C\": np.random.uniform(config.get(\"min_reparam\", 0.5),\n",
        "                               config.get(\"max_reparam\", 0.8)),\n",
        "        \"l1_ratio\": np.random.uniform(config.get(\"min_elasticnet\", 0.5),\n",
        "                                     config.get(\"max_elasticnet\", 0.8)),\n",
        "        \"solver\": 'saga',  # Only solver that supports elasticnet\n",
        "        \"penalty\": 'elasticnet',\n",
        "        \"random_state\": 42\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        model = LogisticRegression(**params)\n",
        "        message = \"LogisticRegression model created successfully\"\n",
        "        return model, message\n",
        "    except Exception as e:\n",
        "        return None, f\"Error creating LogisticRegression model: {str(e)}\"\n",
        "\n",
        "# Example usage:\n",
        "logistic_config = {\n",
        "    \"model_name\": \"LogisticRegression\",\n",
        "    \"is_selected\": True,\n",
        "    \"parallelism\": 2,\n",
        "    \"min_iter\": 30,\n",
        "    \"max_iter\": 50,\n",
        "    \"min_reparam\": 0.5,\n",
        "    \"max_reparam\": 0.8,\n",
        "    \"min_elasticnet\": 0.5,\n",
        "    \"max_elasticnet\": 0.8\n",
        "}\n",
        "\n",
        "# For classification problem\n",
        "model, message = create_logistic_regression_model(logistic_config, \"classification\")\n",
        "print(message)\n",
        "if model:\n",
        "    print(model.get_params())\n",
        "\n",
        "# For regression problem (will return None)\n",
        "model, message = create_logistic_regression_model(logistic_config, \"regression\")\n",
        "print(message)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBs8mhtBsuX6",
        "outputId": "d5a47054-ea8d-4e47-8567-02cc2b8eb451"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogisticRegression model created successfully\n",
            "{'C': 0.6631697308861249, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': 0.6713760996167615, 'max_iter': 36, 'multi_class': 'deprecated', 'n_jobs': 2, 'penalty': 'elasticnet', 'random_state': 42, 'solver': 'saga', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n",
            "LogisticRegression is not suitable for regression (only for classification)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Run the fit and predict on each model – keep in mind that you need to do hyper parameter tuning i.e., use GridSearchCV**\n"
      ],
      "metadata": {
        "id": "pEpjzzBUu__c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "E8PvTvR_wVsn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parses the model configuration (including hyperparameter ranges)\n",
        "\n",
        "Performs hyperparameter tuning using GridSearchCV\n",
        "\n",
        "Fits the best model\n",
        "\n",
        "Makes predictions\n",
        "\n",
        "Handles both classification and regression tasks appropriate."
      ],
      "metadata": {
        "id": "y-9MDB1lvZXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def create_and_tune_logistic_regression(X_train, y_train, X_test, y_test, config, prediction_type):\n",
        "    \"\"\"\n",
        "    Creates, tunes, fits, and evaluates a LogisticRegression model using GridSearchCV\n",
        "\n",
        "    Parameters:\n",
        "    X_train, y_train: Training data\n",
        "    X_test, y_test: Test data (can be None if just want to fit)\n",
        "    config: Model configuration dictionary\n",
        "    prediction_type: 'classification' or 'regression'\n",
        "\n",
        "    Returns:\n",
        "    dict: Contains best model, predictions, scores, and tuning information\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'model_name': 'LogisticRegression',\n",
        "        'best_model': None,\n",
        "        'best_params': None,\n",
        "        'best_score': None,\n",
        "        'predictions': None,\n",
        "        'test_score': None,\n",
        "        'warning': None\n",
        "    }\n",
        "\n",
        "    # Validate prediction type\n",
        "    if prediction_type != 'classification':\n",
        "        results['warning'] = f\"LogisticRegression not suitable for {prediction_type}, only classification\"\n",
        "        return results\n",
        "\n",
        "    if not config['is_selected']:\n",
        "        results['warning'] = \"Model not selected in configuration\"\n",
        "        return results\n",
        "\n",
        "    # Standardize features (important for regularized models)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    if X_test is not None:\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Create parameter grid for GridSearchCV\n",
        "    param_grid = {\n",
        "        'C': np.linspace(config['min_reparam'], config['max_reparam'], 5),\n",
        "        'l1_ratio': np.linspace(config['min_elasticnet'], config['max_elasticnet'], 5),\n",
        "        'penalty': ['elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [config['min_iter'], config['max_iter']],\n",
        "        'n_jobs': [config['parallelism']]\n",
        "    }\n",
        "\n",
        "    # Create and tune model\n",
        "    try:\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=LogisticRegression(random_state=42),\n",
        "            param_grid=param_grid,\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=config['parallelism']\n",
        "        )\n",
        "\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Store results\n",
        "        results['best_model'] = grid_search.best_estimator_\n",
        "        results['best_params'] = grid_search.best_params_\n",
        "        results['best_score'] = grid_search.best_score_\n",
        "\n",
        "        # Make predictions if test data provided\n",
        "        if X_test is not None and y_test is not None:\n",
        "            y_pred = grid_search.predict(X_test_scaled)\n",
        "            results['predictions'] = y_pred\n",
        "            results['test_score'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    except Exception as e:\n",
        "        results['warning'] = f\"Model training failed: {str(e)}\"\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage:\n",
        "logistic_config = {\n",
        "    \"model_name\": \"LogisticRegression\",\n",
        "    \"is_selected\": True,\n",
        "    \"parallelism\": 2,\n",
        "    \"min_iter\": 30,\n",
        "    \"max_iter\": 50,\n",
        "    \"min_reparam\": 0.5,\n",
        "    \"max_reparam\": 0.8,\n",
        "    \"min_elasticnet\": 0.5,\n",
        "    \"max_elasticnet\": 0.8\n",
        "}\n",
        "\n",
        "# Assuming we have some data\n",
        "# from sklearn.datasets import load_iris\n",
        "# data = load_iris()\n",
        "# X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2)\n",
        "\n",
        "# For demonstration, we'll create dummy data\n",
        "np.random.seed(42)\n",
        "X_train = np.random.rand(100, 4)\n",
        "y_train = np.random.randint(0, 2, 100)\n",
        "X_test = np.random.rand(20, 4)\n",
        "y_test = np.random.randint(0, 2, 20)\n",
        "\n",
        "# Run the tuning and prediction\n",
        "results = create_and_tune_logistic_regression(\n",
        "    X_train, y_train,\n",
        "    X_test, y_test,\n",
        "    logistic_config,\n",
        "    'classification'\n",
        ")\n",
        "\n",
        "print(\"Best parameters:\", results['best_params'])\n",
        "print(\"CV accuracy:\", results['best_score'])\n",
        "print(\"Test accuracy:\", results['test_score'])\n",
        "print(\"Predictions:\", results['predictions'])\n",
        "if results['warning']:\n",
        "    print(\"Warning:\", results['warning'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uPkxytIv1x0",
        "outputId": "2a046e8f-66ff-477a-b32f-e6d72bfdb2ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': np.float64(0.5), 'l1_ratio': np.float64(0.7250000000000001), 'max_iter': 30, 'n_jobs': 2, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
            "CV accuracy: 0.5199999999999999\n",
            "Test accuracy: 0.75\n",
            "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6) Log to the console the standard model metrics that apply**"
      ],
      "metadata": {
        "id": "49KPM8e9xnJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, confusion_matrix,\n",
        "                            mean_squared_error, mean_absolute_error, r2_score)\n",
        "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
        "import sys\n",
        "\n",
        "def log_metrics(y_true, y_pred, y_proba=None, prediction_type='classification'):\n",
        "    \"\"\"Log appropriate metrics based on prediction type\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"MODEL EVALUATION METRICS\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    if prediction_type == 'classification':\n",
        "        # Classification metrics\n",
        "        print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
        "        print(f\"Precision (macro avg): {precision_score(y_true, y_pred, average='macro'):.4f}\")\n",
        "        print(f\"Recall (macro avg): {recall_score(y_true, y_pred, average='macro'):.4f}\")\n",
        "        print(f\"F1 Score (macro avg): {f1_score(y_true, y_pred, average='macro'):.4f}\")\n",
        "\n",
        "        if y_proba is not None:\n",
        "            # Handle binary and   multiclass ROC AUC\n",
        "            if len(np.unique(y_true)) == 2:\n",
        "                print(f\"ROC AUC: {roc_auc_score(y_true, y_proba[:, 1]):.4f}\")\n",
        "            else:\n",
        "                # One-vs-rest approach for multiclass\n",
        "                lb = LabelBinarizer()\n",
        "                y_true_bin = lb.fit_transform(y_true)\n",
        "                print(f\"ROC AUC (ovr): {roc_auc_score(y_true_bin, y_proba, multi_class='ovr'):.4f}\")\n",
        "\n",
        "        # Confusion matrix\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "    else:\n",
        "        # Regression metrics\n",
        "        print(f\"Mean Squared Error: {mean_squared_error(y_true, y_pred):.4f}\")\n",
        "        print(f\"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_true, y_pred)):.4f}\")\n",
        "        print(f\"Mean Absolute Error: {mean_absolute_error(y_true, y_pred):.4f}\")\n",
        "        print(f\"R² Score: {r2_score(y_true, y_pred):.4f}\")\n",
        "\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "def create_and_tune_logistic_regression(X_train, y_train, X_test, y_test, config, prediction_type):\n",
        "    \"\"\"\n",
        "    Creates, tunes, fits, and evaluates a LogisticRegression model using GridSearchCV\n",
        "    with comprehensive metric logging\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'model_name': 'LogisticRegression',\n",
        "        'best_model': None,\n",
        "        'best_params': None,\n",
        "        'best_score': None,\n",
        "        'predictions': None,\n",
        "        'test_score': None,\n",
        "        'warning': None\n",
        "    }\n",
        "\n",
        "    # Checking prediction type\n",
        "    if prediction_type != 'classification':\n",
        "        results['warning'] = f\"LogisticRegression not suitable for {prediction_type}, only classification\"\n",
        "        print(results['warning'])\n",
        "        return results\n",
        "\n",
        "    if not config['is_selected']:\n",
        "        results['warning'] = \"Model not selected in configuration\"\n",
        "        print(results['warning'])\n",
        "        return results\n",
        "\n",
        "    # Standardize features\n",
        "    print(\"\\nPreprocessing: Standardizing features...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    if X_test is not None:\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Create parameter grid\n",
        "    param_grid = {\n",
        "        'C': np.linspace(config['min_reparam'], config['max_reparam'], 5),\n",
        "        'l1_ratio': np.linspace(config['min_elasticnet'], config['max_elasticnet'], 5),\n",
        "        'penalty': ['elasticnet'],\n",
        "        'solver': ['saga'],\n",
        "        'max_iter': [config['min_iter'], config['max_iter']],\n",
        "        'n_jobs': [config['parallelism']],\n",
        "        'random_state': [42]\n",
        "    }\n",
        "\n",
        "    print(\"\\nStarting GridSearchCV for Logistic Regression...\")\n",
        "    print(f\"Parameter grid: {param_grid}\")\n",
        "\n",
        "    try:\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=LogisticRegression(),\n",
        "            param_grid=param_grid,\n",
        "            cv=5,\n",
        "            scoring='accuracy',\n",
        "            n_jobs=config['parallelism'],\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"\\nFitting model...\")\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Store results\n",
        "        results['best_model'] = grid_search.best_estimator_\n",
        "        results['best_params'] = grid_search.best_params_\n",
        "        results['best_score'] = grid_search.best_score_\n",
        "\n",
        "        print(\"\\nBest parameters found:\")\n",
        "        for param, value in grid_search.best_params_.items():\n",
        "            print(f\"{param}: {value}\")\n",
        "        print(f\"Cross-validated accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "        # Make predictions if test data provided\n",
        "        if X_test is not None and y_test is not None:\n",
        "            print(\"\\nEvaluating on test set...\")\n",
        "            y_pred = grid_search.predict(X_test_scaled)\n",
        "            y_proba = grid_search.predict_proba(X_test_scaled) if hasattr(grid_search, 'predict_proba') else None\n",
        "\n",
        "            results['predictions'] = y_pred\n",
        "            results['test_score'] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            # Log metrics\n",
        "            log_metrics(y_test, y_pred, y_proba, prediction_type)\n",
        "\n",
        "    except Exception as e:\n",
        "        results['warning'] = f\"Model training failed: {str(e)}\"\n",
        "        print(results['warning'], file=sys.stderr)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    logistic_config = {\n",
        "        \"model_name\": \"LogisticRegression\",\n",
        "        \"is_selected\": True,\n",
        "        \"parallelism\": 2,\n",
        "        \"min_iter\": 30,\n",
        "        \"max_iter\": 50,\n",
        "        \"min_reparam\": 0.5,\n",
        "        \"max_reparam\": 0.8,\n",
        "        \"min_elasticnet\": 0.5,\n",
        "        \"max_elasticnet\": 0.8\n",
        "    }\n",
        "\n",
        "    # Create sample classification data\n",
        "    from sklearn.datasets import make_classification\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=10, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"STARTING LOGISTIC REGRESSION PIPELINE\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    results = create_and_tune_logistic_regression(\n",
        "        X_train, y_train,\n",
        "        X_test, y_test,\n",
        "        logistic_config,\n",
        "        'classification'\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lst3N8XxgwM",
        "outputId": "17c39ca7-aabe-4e29-cdee-b9770b9222a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "STARTING LOGISTIC REGRESSION PIPELINE\n",
            "==================================================\n",
            "\n",
            "Preprocessing: Standardizing features...\n",
            "\n",
            "Starting GridSearchCV for Logistic Regression...\n",
            "Parameter grid: {'C': array([0.5  , 0.575, 0.65 , 0.725, 0.8  ]), 'l1_ratio': array([0.5  , 0.575, 0.65 , 0.725, 0.8  ]), 'penalty': ['elasticnet'], 'solver': ['saga'], 'max_iter': [30, 50], 'n_jobs': [2], 'random_state': [42]}\n",
            "\n",
            "Fitting model...\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "\n",
            "Best parameters found:\n",
            "C: 0.5\n",
            "l1_ratio: 0.575\n",
            "max_iter: 30\n",
            "n_jobs: 2\n",
            "penalty: elasticnet\n",
            "random_state: 42\n",
            "solver: saga\n",
            "Cross-validated accuracy: 0.6700\n",
            "\n",
            "Evaluating on test set...\n",
            "\n",
            "==================================================\n",
            "MODEL EVALUATION METRICS\n",
            "==================================================\n",
            "Accuracy: 0.7400\n",
            "Precision (macro avg): 0.7422\n",
            "Recall (macro avg): 0.7392\n",
            "F1 Score (macro avg): 0.7383\n",
            "ROC AUC (ovr): 0.9003\n",
            "\n",
            "Confusion Matrix:\n",
            "[[42 10  9]\n",
            " [16 61  4]\n",
            " [13  0 45]]\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxUMbmRSx4u1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}